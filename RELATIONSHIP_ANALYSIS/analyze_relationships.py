#!/usr/bin/env python3
"""
VRC Nexus å…³ç³»åˆ†æå·¥å…· V2.1 - å¸¦é—å¿˜æœºåˆ¶
=======================================
æ”¯æŒ"æœ‰æ•ˆæ—¶é•¿"è®¡ç®—ï¼Œè¿‘æœŸäº’åŠ¨æ¯”å†å²äº’åŠ¨ä¿ç•™æ›´å¤šã€‚

ä½¿ç”¨æ–¹æ³•ï¼š
    python analyze_relationships.py --db VRCX.sqlite3
    python analyze_relationships.py --db VRCX.sqlite3 --halflife 180
    python analyze_relationships.py --db VRCX.sqlite3 --halflife auto --recent auto
    python analyze_relationships.py --db VRCX.sqlite3 -r
    python analyze_relationships.py --db VRCX.sqlite3 -r usr
    
åŠè¡°æœŸé€‰é¡¹ (--halflife)ï¼š
    60     çŸ­æœŸè®°å¿†ï¼Œå¼ºè°ƒè¿‘æœŸäº’åŠ¨
    120    ä¸­æœŸè®°å¿†
    180    é•¿æœŸè®°å¿†ï¼Œæ›´çœ‹é‡å†å²
    365    å‡ ä¹ä¸è¡°å‡
    auto   è‡ªé€‚åº”ï¼š90 Ã— (2 - æ´»è·ƒåº¦)ï¼ŒèŒƒå›´90-180å¤©

è¿‘æœŸçª—å£é€‰é¡¹ (--recent)ï¼š
    30     å›ºå®š30å¤©çª—å£
    45     å›ºå®š45å¤©çª—å£
    60     å›ºå®š60å¤©çª—å£
    auto   è‡ªé€‚åº”ï¼š30 + (1 - æ´»è·ƒåº¦) Ã— 30ï¼ŒèŒƒå›´30-60å¤©

å¯¼å‡ºé€‰é¡¹ï¼š
    -r, --export-rankings           å¯¼å‡ºä¸¤ä¸ªæ’åCSVæ–‡ä»¶
    -r usr, --export-rankings usr   å¯¼å‡ºå¸¦å‰ç¼€çš„æ’åCSVæ–‡ä»¶
"""

from __future__ import annotations

import argparse
import os
import sqlite3
from pathlib import Path
from datetime import datetime
from typing import Dict, Set, Tuple, Optional
import sys

try:
    import pandas as pd
    import numpy as np
except ImportError:
    print("é”™è¯¯ï¼šéœ€è¦å®‰è£… pandas å’Œ numpy")
    print("è¿è¡Œï¼špip install pandas numpy")
    sys.exit(1)

EXCLUDED_IDS = {"usr_00000000-0000-0000-0000-000000000000"}


class AnalysisError(RuntimeError):
    """åˆ†æé”™è¯¯"""


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="åˆ†æ VRCX å¥½å‹å…³ç³»ï¼ˆå¸¦é—å¿˜æœºåˆ¶ç‰ˆæœ¬ï¼‰",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
å‚æ•°è¯´æ˜ï¼š
  --halflife: åŠè¡°æœŸï¼ˆå†å²äº’åŠ¨çš„"é—å¿˜é€Ÿåº¦"ï¼‰
    auto   æ ¹æ®æ´»è·ƒåº¦è‡ªåŠ¨è®¡ç®—ï¼š90Ã—(2-æ´»è·ƒåº¦)ï¼ŒèŒƒå›´90-180å¤©
    60-365 æ‰‹åŠ¨æŒ‡å®šå¤©æ•°
    
  --recent: è¿‘æœŸçª—å£ï¼ˆè®¡ç®—"è¿‘æœŸäº²å¯†åº¦"çš„æ—¶é—´èŒƒå›´ï¼‰
    auto   æ ¹æ®æ´»è·ƒåº¦è‡ªåŠ¨è®¡ç®—ï¼š30+(1-æ´»è·ƒåº¦)Ã—30ï¼ŒèŒƒå›´30-60å¤©
    30-60  æ‰‹åŠ¨æŒ‡å®šå¤©æ•°
"""
    )
    parser.add_argument("--db", default="VRCX.sqlite3", help="æ•°æ®åº“è·¯å¾„")
    parser.add_argument("--win", action="store_true", help="ä½¿ç”¨ Windows é»˜è®¤è·¯å¾„")
    parser.add_argument("--output", "-o", default="relationship_report.md", help="è¾“å‡ºæŠ¥å‘Š")
    parser.add_argument("--export-rankings", "-r", nargs='?', const='', default=None, 
                        help="å¯¼å‡ºä¸¤ä¸ªæ’åCSVï¼Œå¯é€‰å‰ç¼€ï¼Œå¦‚ï¼š-r usr")
    parser.add_argument("--top", "-n", type=int, default=25, help="æ˜¾ç¤ºå‰ N å")
    parser.add_argument("--prefix", help="æ•°æ®è¡¨å‰ç¼€")
    parser.add_argument(
        "--halflife",
        default="auto",
        help="åŠè¡°æœŸå¤©æ•°ï¼Œæˆ– 'auto' è‡ªé€‚åº” (é»˜è®¤: auto)"
    )
    parser.add_argument(
        "--recent",
        default="auto",
        help="è¿‘æœŸçª—å£å¤©æ•°ï¼Œæˆ– 'auto' è‡ªé€‚åº” (é»˜è®¤: auto)"
    )
    return parser.parse_args()


def resolve_db_path(args: argparse.Namespace) -> Path:
    if args.win:
        appdata = os.environ.get("APPDATA")
        if not appdata:
            raise AnalysisError("APPDATA ç¯å¢ƒå˜é‡æœªè®¾ç½®")
        path = Path(appdata) / "VRCX" / "VRCX.sqlite3"
        if not path.exists():
            raise AnalysisError(f"æ•°æ®åº“ä¸å­˜åœ¨: {path}")
        return path
    return Path(args.db).expanduser()


def detect_prefix(conn: sqlite3.Connection, explicit_prefix: Optional[str]) -> str:
    if explicit_prefix:
        for suffix in ("_friend_log_current", "_mutual_graph_links"):
            if explicit_prefix.endswith(suffix):
                return explicit_prefix[:-len(suffix)]
        return explicit_prefix
    
    cursor = conn.cursor()
    cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name LIKE '%_friend_log_current'")
    rows = cursor.fetchall()
    
    prefixes = [name[:-len("_friend_log_current")] for (name,) in rows if name.endswith("_friend_log_current")]
    
    if not prefixes:
        raise AnalysisError("æœªæ‰¾åˆ°å¥½å‹æ•°æ®è¡¨")
    if len(prefixes) > 1:
        raise AnalysisError(f"å‘ç°å¤šä¸ªç”¨æˆ·ï¼Œè¯·ç”¨ --prefix æŒ‡å®š: {', '.join(prefixes)}")
    return prefixes[0]


def get_self_user_id(prefix: str) -> str:
    if prefix.startswith("usr"):
        raw = prefix[3:]
        if len(raw) == 32:
            return f"usr_{raw[:8]}-{raw[8:12]}-{raw[12:16]}-{raw[16:20]}-{raw[20:]}"
    return ""


class RelationshipAnalyzerV2:
    def __init__(self, conn: sqlite3.Connection, prefix: str, halflife: str, recent: str = "auto"):
        self.conn = conn
        self.prefix = prefix
        self.self_user_id = get_self_user_id(prefix)
        self.halflife_setting = halflife
        self.recent_setting = recent
        self.halflife: float = 120.0
        self.recent_window: int = 30
        self.activity_factor: float = 0.5
        self.max_date: Optional[datetime] = None
        self.total_days: int = 0
        self.friend_ids: Set[str] = set()
        self.friend_names: Dict[str, str] = {}
    
    def load_friend_list(self) -> int:
        query = f"SELECT user_id, display_name FROM {self.prefix}_friend_log_current"
        df = pd.read_sql_query(query, self.conn)
        self.friend_ids = set(df['user_id'].tolist())
        self.friend_names = dict(zip(df['user_id'], df['display_name']))
        return len(self.friend_ids)
    
    def get_date_range(self) -> Tuple[datetime, int]:
        cursor = self.conn.cursor()
        cursor.execute("SELECT MAX(date(created_at)), MIN(date(created_at)) FROM gamelog_join_leave")
        max_str, min_str = cursor.fetchone()
        self.max_date = datetime.strptime(max_str, '%Y-%m-%d')
        min_date = datetime.strptime(min_str, '%Y-%m-%d')
        self.total_days = (self.max_date - min_date).days + 1
        return self.max_date, self.total_days
    
    def set_adaptive_params(self) -> dict:
        """è®¾ç½®åŠè¡°æœŸå’Œè¿‘æœŸçª—å£ï¼ˆæ”¯æŒ auto æˆ–æ‰‹åŠ¨æŒ‡å®šï¼‰"""
        result = {}
        
        # å…ˆè®¡ç®—æ´»è·ƒåº¦å› å­ï¼ˆä¸¤ä¸ªå‚æ•°éƒ½å¯èƒ½éœ€è¦ï¼‰
        cursor = self.conn.cursor()
        cursor.execute("SELECT COUNT(DISTINCT date(created_at)) FROM gamelog_location")
        my_active_days = cursor.fetchone()[0]
        self.activity_factor = min(my_active_days / self.total_days, 1.0)
        result['my_active_days'] = my_active_days
        result['total_days'] = self.total_days
        result['activity_factor'] = self.activity_factor
        
        # è®¾ç½®åŠè¡°æœŸ
        if self.halflife_setting == 'auto':
            self.halflife = 90 * (2 - self.activity_factor)
            result['halflife_mode'] = 'auto'
        else:
            try:
                self.halflife = float(self.halflife_setting)
                result['halflife_mode'] = 'manual'
            except ValueError:
                raise AnalysisError(f"æ— æ•ˆçš„åŠè¡°æœŸè®¾ç½®: {self.halflife_setting}")
        result['final_halflife'] = self.halflife
        
        # è®¾ç½®è¿‘æœŸçª—å£
        if self.recent_setting == 'auto':
            self.recent_window = int(30 + (1 - self.activity_factor) * 30)
            result['recent_mode'] = 'auto'
        else:
            try:
                self.recent_window = int(self.recent_setting)
                result['recent_mode'] = 'manual'
            except ValueError:
                raise AnalysisError(f"æ— æ•ˆçš„è¿‘æœŸçª—å£è®¾ç½®: {self.recent_setting}")
        result['recent_window'] = self.recent_window
        
        return result
    
    def get_daily_interactions(self) -> pd.DataFrame:
        query = """
        SELECT user_id, date(created_at) as day, SUM(CASE WHEN time > 0 THEN time ELSE 0 END) / 3600000.0 as hours
        FROM gamelog_join_leave
        WHERE type = 'OnPlayerLeft'
        GROUP BY user_id, date(created_at)
        """
        df = pd.read_sql_query(query, self.conn)
        df = df[df['user_id'].isin(self.friend_ids)]
        if self.self_user_id:
            df = df[df['user_id'] != self.self_user_id]
        df['name'] = df['user_id'].map(self.friend_names)
        df['day'] = pd.to_datetime(df['day'])
        return df
    
    def calculate_effective_hours(self, daily_df: pd.DataFrame) -> pd.DataFrame:
        """è®¡ç®—æœ‰æ•ˆæ—¶é•¿ï¼ˆå¸¦è¡°å‡ï¼‰
        
        é‡è¦ï¼šä½¿ç”¨ user_id ä½œä¸ºä¸»é”®ï¼Œé¿å…åŒåå¥½å‹æ•°æ®è¢«è¦†ç›–
        """
        results = []
        
        for user_id in daily_df['user_id'].unique():
            friend_data = daily_df[daily_df['user_id'] == user_id]
            
            effective = 0.0
            total = 0.0
            
            for _, row in friend_data.iterrows():
                days_ago = (self.max_date - row['day']).days
                weight = 2 ** (-days_ago / self.halflife)
                effective += row['hours'] * weight
                total += row['hours']
            
            results.append({
                'user_id': user_id,
                'total_hours': total,
                'effective_hours': effective,
                'retention_rate': effective / total if total > 0 else 0
            })
        
        return pd.DataFrame(results)
    
    def get_friend_stats(self) -> pd.DataFrame:
        query = """
        SELECT user_id, 
               COUNT(*) as interaction_count,
               COUNT(DISTINCT location) as meet_count,
               COUNT(DISTINCT date(created_at)) as active_days
        FROM gamelog_join_leave 
        WHERE type = 'OnPlayerLeft' AND time > 0
        GROUP BY user_id
        """
        df = pd.read_sql_query(query, self.conn)
        df = df[df['user_id'].isin(self.friend_ids)]
        if self.self_user_id:
            df = df[df['user_id'] != self.self_user_id]
        df['name'] = df['user_id'].map(self.friend_names)
        return df
    
    def get_recent_stats(self) -> pd.DataFrame:
        query = f"""
        SELECT user_id, 
               SUM(time) / 3600000.0 as recent_hours, 
               COUNT(DISTINCT location) as recent_meets
        FROM gamelog_join_leave 
        WHERE type = 'OnPlayerLeft' AND time > 0
            AND created_at >= datetime((SELECT MAX(date(created_at)) FROM gamelog_join_leave), '-{self.recent_window} days')
        GROUP BY user_id
        """
        df = pd.read_sql_query(query, self.conn)
        df = df[df['user_id'].isin(self.friend_ids)]
        return df
    
    def get_mutual_friends(self) -> pd.DataFrame:
        query = f"SELECT friend_id as user_id, COUNT(*) as connections FROM {self.prefix}_mutual_graph_links GROUP BY friend_id"
        df = pd.read_sql_query(query, self.conn)
        df = df[df['user_id'].isin(self.friend_ids)]
        return df
    
    def get_my_recent_hours(self) -> float:
        query = f"""
        SELECT SUM(time) / 3600000.0
        FROM gamelog_location
        WHERE created_at >= datetime((SELECT MAX(date(created_at)) FROM gamelog_location), '-{self.recent_window} days')
        """
        cursor = self.conn.cursor()
        cursor.execute(query)
        result = cursor.fetchone()[0]
        return result if result else 0
    
    def calculate_relationship_strength(self, df: pd.DataFrame) -> pd.DataFrame:
        """å…³ç³»å¼ºåº¦ V2 - ä½¿ç”¨æœ‰æ•ˆæ—¶é•¿ï¼Œå«éšè—å¥½å‹æ£€æµ‹"""
        result = df.copy()
        
        # æœ‰æ•ˆé™ªä¼´æ·±åº¦ (40%)
        result['depth_percentile'] = result['effective_hours'].rank(pct=True)
        result['depth_score'] = result['depth_percentile'] * 40
        
        # äº’åŠ¨è´¨é‡ (25%) - ä½¿ç”¨ interaction_count è®¡ç®—å¹³å‡æ¯æ¬¡äº’åŠ¨æ—¶é•¿
        result['avg_duration'] = result['total_hours'] / result['interaction_count']
        median_duration = result['avg_duration'].median()
        result['quality_score'] = (result['avg_duration'] / (result['avg_duration'] + median_duration)) * 25
        
        # ç¨³å®šæ€§ (20%)
        result['stability_ratio'] = result['active_days'] / self.total_days
        result['stability_score'] = np.sqrt(result['stability_ratio']) * 20
        
        # ç¤¾äº¤ç¾ç»Š (15%) - å«éšè—å¥½å‹åŠ¨æ€è¯†åˆ«
        result['is_hidden_friend'] = False
        result['bond_score'] = 7.5
        
        if 'connections' in result.columns:
            # æ­£å¸¸å¥½å‹
            has_connections = result['connections'] > 0
            if has_connections.any():
                result.loc[has_connections, 'bond_percentile'] = result.loc[has_connections, 'connections'].rank(pct=True)
                result.loc[has_connections, 'bond_score'] = result.loc[has_connections, 'bond_percentile'] * 15
            
            # éšè—å¥½å‹æ£€æµ‹ï¼ˆä½¿ç”¨æ€»æ—¶é•¿ï¼Œä¸å—è¡°å‡å½±å“ï¼‰
            zero_conn = result['connections'] == 0
            hours_p70 = result['total_hours'].quantile(0.70)
            meets_p70 = result['meet_count'].quantile(0.70)
            high_interaction = (result['total_hours'] > hours_p70) | (result['meet_count'] > meets_p70)
            hidden = zero_conn & high_interaction
            
            result.loc[hidden, 'is_hidden_friend'] = True
            result.loc[hidden, 'bond_score'] = result.loc[hidden, 'depth_percentile'] * 15
        
        result['relationship_strength'] = (
            result['depth_score'] + result['quality_score'] + 
            result['stability_score'] + result['bond_score']
        )
        return result
    
    def calculate_recent_intimacy(self, df: pd.DataFrame, my_recent_hours: float) -> pd.DataFrame:
        result = df.copy()
        result['recent_hours'] = result['recent_hours'].fillna(0)
        result['recent_meets'] = result['recent_meets'].fillna(0)
        
        has_recent = result['recent_hours'] > 0
        
        result['recent_time_score'] = 0.0
        if has_recent.any():
            result.loc[has_recent, 'recent_time_score'] = result.loc[has_recent, 'recent_hours'].rank(pct=True) * 40
        
        result['recent_freq_score'] = 0.0
        if has_recent.any():
            result.loc[has_recent, 'recent_freq_score'] = result.loc[has_recent, 'recent_meets'].rank(pct=True) * 30
        
        result['life_share'] = 0.0
        result['share_score'] = 0.0
        if my_recent_hours > 0:
            result['life_share'] = result['recent_hours'] / my_recent_hours
            share_median = result.loc[has_recent, 'life_share'].median() if has_recent.any() else 0.01
            result['share_score'] = (result['life_share'] / (result['life_share'] + max(share_median, 0.01))) * 30
        
        result['recent_intimacy'] = result['recent_time_score'] + result['recent_freq_score'] + result['share_score']
        return result
    
    def analyze(self) -> Tuple[pd.DataFrame, dict]:
        print("åŠ è½½å¥½å‹åˆ—è¡¨...")
        friend_count = self.load_friend_list()
        print(f"  å¥½å‹æ•°é‡: {friend_count}")
        
        print("è·å–æ•°æ®èŒƒå›´...")
        max_date, total_days = self.get_date_range()
        print(f"  æ•°æ®èŒƒå›´: {total_days} å¤©ï¼Œæˆªè‡³ {max_date.strftime('%Y-%m-%d')}")
        
        print("è®¡ç®—åŠè¡°æœŸå’Œè¿‘æœŸçª—å£...")
        params_info = self.set_adaptive_params()
        
        # æ˜¾ç¤ºæ´»è·ƒåº¦ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ä»»ä¸€ä¸ªæ˜¯ autoï¼‰
        if self.halflife_setting == 'auto' or self.recent_setting == 'auto':
            print(f"  æˆ‘çš„æ´»è·ƒå¤©æ•°: {params_info['my_active_days']} / {params_info['total_days']} å¤©")
            print(f"  æ´»è·ƒåº¦å› å­: {params_info['activity_factor']:.2f}")
        
        # æ˜¾ç¤ºåŠè¡°æœŸ
        if params_info.get('halflife_mode') == 'auto':
            print(f"  åŠè¡°æœŸ: 90 Ã— (2 - {params_info['activity_factor']:.2f}) = {self.halflife:.0f} å¤© [auto]")
        else:
            print(f"  åŠè¡°æœŸ: {self.halflife:.0f} å¤© [æ‰‹åŠ¨æŒ‡å®š]")
        
        # æ˜¾ç¤ºè¿‘æœŸçª—å£
        if params_info.get('recent_mode') == 'auto':
            print(f"  è¿‘æœŸçª—å£: 30 + (1 - {params_info['activity_factor']:.2f}) Ã— 30 = {self.recent_window} å¤© [auto]")
        else:
            print(f"  è¿‘æœŸçª—å£: {self.recent_window} å¤© [æ‰‹åŠ¨æŒ‡å®š]")
        
        print(f"  >>> åŠè¡°æœŸ: {self.halflife:.0f} å¤© | è¿‘æœŸçª—å£: {self.recent_window} å¤© <<<")
        print(f"  å«ä¹‰: {self.halflife:.0f}å¤©å‰çš„1å°æ—¶ = ç°åœ¨çš„0.5å°æ—¶")
        
        print("è·å–äº’åŠ¨æ•°æ®...")
        daily_df = self.get_daily_interactions()
        friend_stats = self.get_friend_stats()
        recent_stats = self.get_recent_stats()
        mutual_friends = self.get_mutual_friends()
        my_recent_hours = self.get_my_recent_hours()
        
        print("è®¡ç®—æœ‰æ•ˆæ—¶é•¿...")
        effective_df = self.calculate_effective_hours(daily_df)
        print(f"  æœ‰äº’åŠ¨è®°å½•çš„å¥½å‹: {len(effective_df)}")
        print(f"  æˆ‘è¿‘{self.recent_window}å¤©åœ¨çº¿: {my_recent_hours:.1f} å°æ—¶")
        
        # åˆå¹¶ï¼ˆå…¨éƒ¨ä½¿ç”¨ user_id ä½œä¸ºä¸»é”®ï¼Œé¿å…åŒåå¥½å‹é—®é¢˜ï¼‰
        df = friend_stats.merge(effective_df, on='user_id', how='inner')
        df = df.merge(recent_stats, on='user_id', how='left')
        df = df.merge(mutual_friends, on='user_id', how='left')
        df['connections'] = df['connections'].fillna(0)
        
        print("è®¡ç®—å…³ç³»å¼ºåº¦ V2...")
        df = self.calculate_relationship_strength(df)
        
        print("è®¡ç®—è¿‘æœŸäº²å¯†åº¦...")
        df = self.calculate_recent_intimacy(df, my_recent_hours)
        
        params_info['recent_window'] = self.recent_window
        return df, params_info


def generate_report(df: pd.DataFrame, total_days: int, halflife: float, halflife_info: dict, top_n: int) -> str:
    recent_window = halflife_info.get('recent_window', 30)
    lines = []
    lines.append("=" * 70)
    lines.append("VRC Nexus å…³ç³»åˆ†ææŠ¥å‘Š V2.1 - å¸¦é—å¿˜æœºåˆ¶")
    lines.append("=" * 70)
    lines.append(f"\næ•°æ®èŒƒå›´ï¼š{total_days} å¤©")
    lines.append(f"å¥½å‹æ€»æ•°ï¼š{len(df)} äºº")
    lines.append(f"åŠè¡°æœŸï¼š{halflife:.0f} å¤© | è¿‘æœŸçª—å£ï¼š{recent_window} å¤©")
    lines.append(f"æŠ¥å‘Šæ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M')}")
    
    # å…³ç³»å¼ºåº¦æ’å
    df_strength = df.sort_values('relationship_strength', ascending=False).head(top_n)
    lines.append("\n" + "=" * 70)
    lines.append("ã€å…³ç³»å¼ºåº¦æ’å V2ã€‘- ä½¿ç”¨æœ‰æ•ˆæ—¶é•¿ï¼ˆå¸¦é—å¿˜ï¼‰")
    lines.append("=" * 70)
    lines.append(f"\n{'æ’å':<4} {'å¥½å‹':<18} {'æ€»æ—¶é•¿':<8} {'æœ‰æ•ˆæ—¶é•¿':<8} {'ä¿ç•™ç‡':<8} {'å¼ºåº¦%':<6} {'æ ‡è®°':<6}")
    lines.append("-" * 75)
    
    for i, row in enumerate(df_strength.itertuples(), 1):
        retention = row.retention_rate * 100
        mark = "éšè—" if (hasattr(row, 'is_hidden_friend') and row.is_hidden_friend) else ""
        lines.append(
            f"{i:<4} {row.name:<18} {row.total_hours:>6.1f}h  {row.effective_hours:>6.1f}h  "
            f"{retention:>5.1f}%   {row.relationship_strength:>5.1f}%  {mark}"
        )
    
    # è¿‘æœŸäº²å¯†åº¦æ’å
    df_recent = df.sort_values('recent_intimacy', ascending=False).head(top_n)
    lines.append("\n" + "=" * 70)
    lines.append(f"ã€è¿‘æœŸäº²å¯†åº¦æ’åã€‘- è¿‘ {recent_window} å¤©")
    lines.append("=" * 70)
    lines.append(f"\n{'æ’å':<4} {'å¥½å‹':<18} {'è¿‘æœŸh':<8} {'è§é¢æ¬¡':<6} {'ç”Ÿå‘½ä»½é¢':<10} {'äº²å¯†åº¦':<6}")
    lines.append("-" * 70)
    
    for i, row in enumerate(df_recent.itertuples(), 1):
        share_pct = row.life_share * 100 if pd.notna(row.life_share) else 0
        recent_hours = row.recent_hours if pd.notna(row.recent_hours) else 0
        recent_meets = int(row.recent_meets) if pd.notna(row.recent_meets) else 0
        lines.append(
            f"{i:<4} {row.name:<18} {recent_hours:>6.1f}h  {recent_meets:>4}æ¬¡   "
            f"{share_pct:>6.2f}%     {row.recent_intimacy:>5.1f}"
        )
    
    # éšè—å¥½å‹æ£€æµ‹
    if 'is_hidden_friend' in df.columns:
        hidden = df[df['is_hidden_friend'] == True].sort_values('total_hours', ascending=False)
        if len(hidden) > 0:
            lines.append(f"\nğŸ”’ æ£€æµ‹åˆ°çš„éšè—å¥½å‹ï¼ˆå…±åŒå¥½å‹=0 ä½†äº’åŠ¨é‡é«˜ï¼‰ï¼š")
            for row in hidden.itertuples():
                lines.append(f"   - {row.name}: {row.total_hours:.1f}h, {row.meet_count}æ¬¡è§é¢")
    
    # ä¿ç•™ç‡åˆ†æ
    lines.append("\n" + "=" * 70)
    lines.append("ã€æœ‰æ•ˆæ—¶é•¿åˆ†æã€‘- é—å¿˜æœºåˆ¶çš„å½±å“")
    lines.append("=" * 70)
    
    # ä¿ç•™ç‡æœ€ä½
    low_retention = df[df['total_hours'] > 30].nsmallest(8, 'retention_rate')
    lines.append("\nğŸ“‰ ä¿ç•™ç‡æœ€ä½ï¼ˆå…³ç³»æ­£åœ¨æ·¡åŒ–ï¼‰ï¼š")
    for row in low_retention.itertuples():
        lines.append(f"   - {row.name}: æ€»{row.total_hours:.0f}h â†’ æœ‰æ•ˆ{row.effective_hours:.1f}h (ä¿ç•™{row.retention_rate*100:.1f}%)")
    
    # ä¿ç•™ç‡æœ€é«˜
    high_retention = df[df['total_hours'] > 20].nlargest(8, 'retention_rate')
    lines.append("\nğŸ“ˆ ä¿ç•™ç‡æœ€é«˜ï¼ˆå…³ç³»å¾ˆæ–°é²œï¼‰ï¼š")
    for row in high_retention.itertuples():
        lines.append(f"   - {row.name}: æ€»{row.total_hours:.0f}h â†’ æœ‰æ•ˆ{row.effective_hours:.1f}h (ä¿ç•™{row.retention_rate*100:.1f}%)")
    
    return "\n".join(lines)


def main():
    args = parse_args()
    db_path = resolve_db_path(args)
    if not db_path.exists():
        raise AnalysisError(f"æ•°æ®åº“ä¸å­˜åœ¨: {db_path}")
    
    print(f"æ•°æ®åº“: {db_path}")
    
    conn = sqlite3.connect(db_path)
    try:
        prefix = detect_prefix(conn, args.prefix)
        print(f"ç”¨æˆ·å‰ç¼€: {prefix}")
        
        analyzer = RelationshipAnalyzerV2(conn, prefix, args.halflife, args.recent)
        df, params_info = analyzer.analyze()
        
        report = generate_report(df, analyzer.total_days, analyzer.halflife, params_info, args.top)
        print("\n" + report)
        
        Path(args.output).write_text(report, encoding='utf-8')
        print(f"\næŠ¥å‘Šå·²ä¿å­˜åˆ°: {args.output}")
        
        # å¯¼å‡ºæ’åCSV
        if args.export_rankings is not None:
            cols = ['name', 'total_hours', 'effective_hours', 'retention_rate', 
                    'meet_count', 'interaction_count', 'active_days', 'connections', 
                    'recent_hours', 'recent_meets', 'relationship_strength', 'recent_intimacy']
            
            # ç”Ÿæˆæ–‡ä»¶åï¼ˆå¸¦å¯é€‰å‰ç¼€ï¼‰
            prefix = f"{args.export_rankings}_" if args.export_rankings else ""
            
            # å…³ç³»å¼ºåº¦æ’å
            strength_file = f'{prefix}relationship_strength_ranking.csv'
            df_strength = df.sort_values('relationship_strength', ascending=False).copy()
            df_strength.insert(0, 'rank', range(1, len(df_strength) + 1))
            df_strength[[c for c in ['rank'] + cols if c in df_strength.columns]].to_csv(strength_file, index=False)
            print(f"å…³ç³»å¼ºåº¦æ’åå·²ä¿å­˜åˆ°: {strength_file}")
            
            # è¿‘æœŸäº²å¯†åº¦æ’å
            intimacy_file = f'{prefix}recent_intimacy_ranking.csv'
            df_intimacy = df.sort_values('recent_intimacy', ascending=False).copy()
            df_intimacy.insert(0, 'rank', range(1, len(df_intimacy) + 1))
            df_intimacy[[c for c in ['rank'] + cols if c in df_intimacy.columns]].to_csv(intimacy_file, index=False)
            print(f"è¿‘æœŸäº²å¯†åº¦æ’åå·²ä¿å­˜åˆ°: {intimacy_file}")
    finally:
        conn.close()


if __name__ == "__main__":
    try:
        main()
    except AnalysisError as e:
        print(f"é”™è¯¯: {e}", file=sys.stderr)
        sys.exit(1)
